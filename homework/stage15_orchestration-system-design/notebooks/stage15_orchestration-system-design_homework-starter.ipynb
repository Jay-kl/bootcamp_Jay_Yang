{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter â€” Stage 15: Orchestration & System Design\n",
    "Complete the sections below. Keep your answers concise and focused on orchestration readiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Project Task Decomposition\n",
    "List 4â€“8 tasks. Add more rows as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Frequency Trading Factor Prediction System - Task Breakdown\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>inputs</th>\n",
       "      <th>outputs</th>\n",
       "      <th>duration_mins</th>\n",
       "      <th>frequency</th>\n",
       "      <th>idempotent</th>\n",
       "      <th>criticality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_ingestion</td>\n",
       "      <td>/data/market/raw_trading_data.csv</td>\n",
       "      <td>data/processed/trading_data_raw.parquet</td>\n",
       "      <td>5</td>\n",
       "      <td>hourly</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_validation</td>\n",
       "      <td>data/processed/trading_data_raw.parquet</td>\n",
       "      <td>data/processed/trading_data_validated.parquet</td>\n",
       "      <td>2</td>\n",
       "      <td>hourly</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_engineering</td>\n",
       "      <td>data/processed/trading_data_validated.parquet</td>\n",
       "      <td>data/features/trading_factors.parquet</td>\n",
       "      <td>10</td>\n",
       "      <td>hourly</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model_training</td>\n",
       "      <td>data/features/trading_factors.parquet</td>\n",
       "      <td>models/trained/regression_model.pkl,models/tra...</td>\n",
       "      <td>30</td>\n",
       "      <td>daily</td>\n",
       "      <td>True</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model_validation</td>\n",
       "      <td>models/trained/regression_model.pkl</td>\n",
       "      <td>models/validated/champion_model.pkl,reports/mo...</td>\n",
       "      <td>15</td>\n",
       "      <td>daily</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model_deployment</td>\n",
       "      <td>models/validated/champion_model.pkl</td>\n",
       "      <td>models/production/live_model.pkl,config/deploy...</td>\n",
       "      <td>10</td>\n",
       "      <td>weekly</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prediction_serving</td>\n",
       "      <td>models/production/live_model.pkl</td>\n",
       "      <td>api/predictions/results.json,logs/predictions/...</td>\n",
       "      <td>1</td>\n",
       "      <td>realtime</td>\n",
       "      <td>False</td>\n",
       "      <td>critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>monitoring_alerts</td>\n",
       "      <td>logs/predictions/*.json</td>\n",
       "      <td>alerts/model_health.json,reports/monitoring_su...</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  task                                         inputs  \\\n",
       "0       data_ingestion              /data/market/raw_trading_data.csv   \n",
       "1      data_validation        data/processed/trading_data_raw.parquet   \n",
       "2  feature_engineering  data/processed/trading_data_validated.parquet   \n",
       "3       model_training          data/features/trading_factors.parquet   \n",
       "4     model_validation            models/trained/regression_model.pkl   \n",
       "5     model_deployment            models/validated/champion_model.pkl   \n",
       "6   prediction_serving               models/production/live_model.pkl   \n",
       "7    monitoring_alerts                        logs/predictions/*.json   \n",
       "\n",
       "                                             outputs  duration_mins  \\\n",
       "0            data/processed/trading_data_raw.parquet              5   \n",
       "1      data/processed/trading_data_validated.parquet              2   \n",
       "2              data/features/trading_factors.parquet             10   \n",
       "3  models/trained/regression_model.pkl,models/tra...             30   \n",
       "4  models/validated/champion_model.pkl,reports/mo...             15   \n",
       "5  models/production/live_model.pkl,config/deploy...             10   \n",
       "6  api/predictions/results.json,logs/predictions/...              1   \n",
       "7  alerts/model_health.json,reports/monitoring_su...              5   \n",
       "\n",
       "    frequency  idempotent criticality  \n",
       "0      hourly        True        high  \n",
       "1      hourly        True        high  \n",
       "2      hourly        True        high  \n",
       "3       daily        True      medium  \n",
       "4       daily        True        high  \n",
       "5      weekly        True        high  \n",
       "6    realtime       False    critical  \n",
       "7  continuous        True        high  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High-Frequency Trading Factor Prediction System - Task Decomposition\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define comprehensive task breakdown for the trading factor prediction pipeline\n",
    "tasks = pd.DataFrame({\n",
    "    'task': [\n",
    "        'data_ingestion',\n",
    "        'data_validation', \n",
    "        'feature_engineering',\n",
    "        'model_training',\n",
    "        'model_validation',\n",
    "        'model_deployment',\n",
    "        'prediction_serving',\n",
    "        'monitoring_alerts'\n",
    "    ],\n",
    "    'inputs': [\n",
    "        '/data/market/raw_trading_data.csv',\n",
    "        'data/processed/trading_data_raw.parquet',\n",
    "        'data/processed/trading_data_validated.parquet', \n",
    "        'data/features/trading_factors.parquet',\n",
    "        'models/trained/regression_model.pkl',\n",
    "        'models/validated/champion_model.pkl',\n",
    "        'models/production/live_model.pkl',\n",
    "        'logs/predictions/*.json'\n",
    "    ],\n",
    "    'outputs': [\n",
    "        'data/processed/trading_data_raw.parquet',\n",
    "        'data/processed/trading_data_validated.parquet',\n",
    "        'data/features/trading_factors.parquet',\n",
    "        'models/trained/regression_model.pkl,models/trained/classification_model.pkl',\n",
    "        'models/validated/champion_model.pkl,reports/model_validation.json',\n",
    "        'models/production/live_model.pkl,config/deployment_manifest.yaml',\n",
    "        'api/predictions/results.json,logs/predictions/prediction_log.json',\n",
    "        'alerts/model_health.json,reports/monitoring_summary.json'\n",
    "    ],\n",
    "    'duration_mins': [5, 2, 10, 30, 15, 10, 1, 5],\n",
    "    'frequency': ['hourly', 'hourly', 'hourly', 'daily', 'daily', 'weekly', 'realtime', 'continuous'],\n",
    "    'idempotent': [True, True, True, True, True, True, False, True],\n",
    "    'criticality': ['high', 'high', 'high', 'medium', 'high', 'high', 'critical', 'high']\n",
    "})\n",
    "\n",
    "print(\"High-Frequency Trading Factor Prediction System - Task Breakdown\")\n",
    "print(\"=\" * 70)\n",
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Dependencies (DAG)\n",
    "Describe dependencies and paste a small diagram if you have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trading Factor Prediction Pipeline - Dependency Graph\n",
      "============================================================\n",
      "ğŸ data_ingestion       â† entry point (no dependencies)\n",
      "âœ“ data_validation      â† depends on: data_ingestion\n",
      "âœ“ feature_engineering  â† depends on: data_validation\n",
      "âœ“ model_training       â† depends on: feature_engineering\n",
      "âœ“ model_validation     â† depends on: model_training\n",
      "âœ“ model_deployment     â† depends on: model_validation\n",
      "âœ“ prediction_serving   â† depends on: model_deployment\n",
      "âœ“ monitoring_alerts    â† depends on: prediction_serving + model_deployment\n",
      "\n",
      "ğŸ”„ Pipeline Flow Visualization:\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ data_ingestion  â”‚ (hourly)\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "          â”‚\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ data_validation â”‚ (hourly) \n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "          â”‚\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚feature_engineer â”‚ (hourly)\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "          â”‚\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ model_training  â”‚ (daily)\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "          â”‚\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚model_validation â”‚ (daily)\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "          â”‚\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚model_deployment â”‚ (weekly)\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "          â”‚\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚prediction_serve â”‚â”€â”€â”€â”€â–¶â”‚monitoring_alerts â”‚\n",
      "â”‚   (realtime)    â”‚     â”‚   (continuous)   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data_ingestion': [],\n",
       " 'data_validation': ['data_ingestion'],\n",
       " 'feature_engineering': ['data_validation'],\n",
       " 'model_training': ['feature_engineering'],\n",
       " 'model_validation': ['model_training'],\n",
       " 'model_deployment': ['model_validation'],\n",
       " 'prediction_serving': ['model_deployment'],\n",
       " 'monitoring_alerts': ['prediction_serving', 'model_deployment']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define complex DAG dependencies for high-frequency trading system\n",
    "dag = {\n",
    "    'data_ingestion': [],  # No dependencies - starts the pipeline\n",
    "    'data_validation': ['data_ingestion'],  # Depends on raw data being ingested\n",
    "    'feature_engineering': ['data_validation'],  # Requires validated data\n",
    "    'model_training': ['feature_engineering'],  # Needs engineered features\n",
    "    'model_validation': ['model_training'],  # Validates trained models\n",
    "    'model_deployment': ['model_validation'],  # Deploy only validated models\n",
    "    'prediction_serving': ['model_deployment'],  # Serve predictions with deployed models\n",
    "    'monitoring_alerts': ['prediction_serving', 'model_deployment']  # Monitor both serving and deployment\n",
    "}\n",
    "\n",
    "print(\"Trading Factor Prediction Pipeline - Dependency Graph\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize the DAG structure\n",
    "for task, dependencies in dag.items():\n",
    "    if dependencies:\n",
    "        deps_str = \" + \".join(dependencies)\n",
    "        print(f\"âœ“ {task:<20} â† depends on: {deps_str}\")\n",
    "    else:\n",
    "        print(f\"ğŸ {task:<20} â† entry point (no dependencies)\")\n",
    "\n",
    "print(\"\\nğŸ”„ Pipeline Flow Visualization:\")\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ data_ingestion  â”‚ (hourly)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ data_validation â”‚ (hourly) \n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚feature_engineer â”‚ (hourly)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ model_training  â”‚ (daily)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚model_validation â”‚ (daily)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚model_deployment â”‚ (weekly)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚prediction_serve â”‚â”€â”€â”€â”€â–¶â”‚monitoring_alerts â”‚\n",
    "â”‚   (realtime)    â”‚     â”‚   (continuous)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Logging & Checkpoints Plan\n",
    "Specify what you will log and where you will checkpoint for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Logging & Checkpoint Strategy\n",
      "=======================================================\n",
      "Key Design Principles:\n",
      "â€¢ Structured JSON logging for all critical events\n",
      "â€¢ Automatic artifact versioning with timestamps\n",
      "â€¢ Distributed backup storage for disaster recovery\n",
      "â€¢ Configurable retention policies based on criticality\n",
      "â€¢ Real-time monitoring integration\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>log_messages</th>\n",
       "      <th>checkpoint_artifact</th>\n",
       "      <th>retention_days</th>\n",
       "      <th>backup_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_ingestion</td>\n",
       "      <td>start/end timestamps, rows ingested, data sour...</td>\n",
       "      <td>data/processed/trading_data_raw.parquet + meta...</td>\n",
       "      <td>30</td>\n",
       "      <td>s3://trading-data/raw/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_validation</td>\n",
       "      <td>start/end, data quality scores, null rates, sc...</td>\n",
       "      <td>data/processed/trading_data_validated.parquet ...</td>\n",
       "      <td>30</td>\n",
       "      <td>s3://trading-data/validated/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_engineering</td>\n",
       "      <td>start/end, feature counts, correlation matrix ...</td>\n",
       "      <td>data/features/trading_factors.parquet + featur...</td>\n",
       "      <td>90</td>\n",
       "      <td>s3://trading-data/features/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model_training</td>\n",
       "      <td>hyperparameters, training metrics (RÂ², RMSE, a...</td>\n",
       "      <td>models/trained/model.pkl + training_metrics.js...</td>\n",
       "      <td>365</td>\n",
       "      <td>s3://trading-models/trained/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model_validation</td>\n",
       "      <td>model performance scores, validation dataset s...</td>\n",
       "      <td>models/validated/champion_model.pkl + validati...</td>\n",
       "      <td>365</td>\n",
       "      <td>s3://trading-models/validated/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model_deployment</td>\n",
       "      <td>deployment status, model version, rollback cap...</td>\n",
       "      <td>models/production/live_model.pkl + deployment_...</td>\n",
       "      <td>180</td>\n",
       "      <td>s3://trading-models/production/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prediction_serving</td>\n",
       "      <td>prediction requests/sec, latency metrics, mode...</td>\n",
       "      <td>predictions/batch_results.json + inference_log...</td>\n",
       "      <td>7</td>\n",
       "      <td>s3://trading-predictions/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>monitoring_alerts</td>\n",
       "      <td>alert triggers, model drift indicators, perfor...</td>\n",
       "      <td>alerts/triggered_alerts.json + monitoring_dash...</td>\n",
       "      <td>90</td>\n",
       "      <td>s3://trading-monitoring/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  task                                       log_messages  \\\n",
       "0       data_ingestion  start/end timestamps, rows ingested, data sour...   \n",
       "1      data_validation  start/end, data quality scores, null rates, sc...   \n",
       "2  feature_engineering  start/end, feature counts, correlation matrix ...   \n",
       "3       model_training  hyperparameters, training metrics (RÂ², RMSE, a...   \n",
       "4     model_validation  model performance scores, validation dataset s...   \n",
       "5     model_deployment  deployment status, model version, rollback cap...   \n",
       "6   prediction_serving  prediction requests/sec, latency metrics, mode...   \n",
       "7    monitoring_alerts  alert triggers, model drift indicators, perfor...   \n",
       "\n",
       "                                 checkpoint_artifact  retention_days  \\\n",
       "0  data/processed/trading_data_raw.parquet + meta...              30   \n",
       "1  data/processed/trading_data_validated.parquet ...              30   \n",
       "2  data/features/trading_factors.parquet + featur...              90   \n",
       "3  models/trained/model.pkl + training_metrics.js...             365   \n",
       "4  models/validated/champion_model.pkl + validati...             365   \n",
       "5  models/production/live_model.pkl + deployment_...             180   \n",
       "6  predictions/batch_results.json + inference_log...               7   \n",
       "7  alerts/triggered_alerts.json + monitoring_dash...              90   \n",
       "\n",
       "                   backup_location  \n",
       "0           s3://trading-data/raw/  \n",
       "1     s3://trading-data/validated/  \n",
       "2      s3://trading-data/features/  \n",
       "3     s3://trading-models/trained/  \n",
       "4   s3://trading-models/validated/  \n",
       "5  s3://trading-models/production/  \n",
       "6        s3://trading-predictions/  \n",
       "7         s3://trading-monitoring/  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprehensive logging and checkpoint strategy for production trading system\n",
    "logging_plan = pd.DataFrame({\n",
    "    'task': [\n",
    "        'data_ingestion',\n",
    "        'data_validation', \n",
    "        'feature_engineering',\n",
    "        'model_training',\n",
    "        'model_validation',\n",
    "        'model_deployment',\n",
    "        'prediction_serving',\n",
    "        'monitoring_alerts'\n",
    "    ],\n",
    "    'log_messages': [\n",
    "        'start/end timestamps, rows ingested, data source health, market session info',\n",
    "        'start/end, data quality scores, null rates, schema validation results, outlier counts',\n",
    "        'start/end, feature counts, correlation matrix hash, drift detection scores, processing time',\n",
    "        'hyperparameters, training metrics (RÂ², RMSE, accuracy), feature importance, convergence status',\n",
    "        'model performance scores, validation dataset stats, A/B test results, business metrics',\n",
    "        'deployment status, model version, rollback capabilities, health check results, traffic routing',\n",
    "        'prediction requests/sec, latency metrics, model inference time, confidence scores, error rates',\n",
    "        'alert triggers, model drift indicators, performance degradation, SLA violations, escalation status'\n",
    "    ],\n",
    "    'checkpoint_artifact': [\n",
    "        'data/processed/trading_data_raw.parquet + metadata.json',\n",
    "        'data/processed/trading_data_validated.parquet + quality_report.json',\n",
    "        'data/features/trading_factors.parquet + feature_metadata.json',\n",
    "        'models/trained/model.pkl + training_metrics.json + hyperparams.yaml',\n",
    "        'models/validated/champion_model.pkl + validation_report.json + performance_metrics.json',\n",
    "        'models/production/live_model.pkl + deployment_manifest.yaml + rollback_config.json',\n",
    "        'predictions/batch_results.json + inference_logs.json + latency_metrics.json',\n",
    "        'alerts/triggered_alerts.json + monitoring_dashboard.json + sla_compliance.json'\n",
    "    ],\n",
    "    'retention_days': [30, 30, 90, 365, 365, 180, 7, 90],\n",
    "    'backup_location': [\n",
    "        's3://trading-data/raw/',\n",
    "        's3://trading-data/validated/',\n",
    "        's3://trading-data/features/',\n",
    "        's3://trading-models/trained/',\n",
    "        's3://trading-models/validated/',\n",
    "        's3://trading-models/production/',\n",
    "        's3://trading-predictions/',\n",
    "        's3://trading-monitoring/'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Comprehensive Logging & Checkpoint Strategy\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Key Design Principles:\")\n",
    "print(\"â€¢ Structured JSON logging for all critical events\")\n",
    "print(\"â€¢ Automatic artifact versioning with timestamps\")\n",
    "print(\"â€¢ Distributed backup storage for disaster recovery\")\n",
    "print(\"â€¢ Configurable retention policies based on criticality\")\n",
    "print(\"â€¢ Real-time monitoring integration\")\n",
    "print()\n",
    "\n",
    "logging_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Right-Sizing Automation\n",
    "Which parts will you automate now? Which stay manual? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation Strategy for High-Frequency Trading Factor System\n",
    "\n",
    "### **Fully Automated Components**\n",
    "\n",
    "**Data Ingestion & Validation (Hourly)**\n",
    "- **Rationale**: High-frequency trading requires consistent, timely data updates. Manual intervention would introduce unacceptable latency and human error risk.\n",
    "- **Implementation**: Automated schedulers (Airflow/Prefect) with built-in retry mechanisms and data quality checks.\n",
    "\n",
    "**Feature Engineering (Hourly)**\n",
    "- **Rationale**: Feature calculations are deterministic and computational. Automation ensures consistency and enables rapid response to market changes.\n",
    "- **Implementation**: Containerized feature processing with automatic scaling based on data volume.\n",
    "\n",
    "**Model Inference/Prediction Serving (Real-time)**\n",
    "- **Rationale**: Trading decisions require sub-second response times. Human intervention is impossible at this frequency.\n",
    "- **Implementation**: Auto-scaling API infrastructure with circuit breakers and fallback mechanisms.\n",
    "\n",
    "**Monitoring & Alerting (Continuous)**\n",
    "- **Rationale**: System health monitoring must be continuous to catch issues before they impact trading performance.\n",
    "- **Implementation**: Automated monitoring with tiered alert escalation and self-healing capabilities.\n",
    "\n",
    "### **Semi-Automated with Human Oversight**\n",
    "\n",
    "**Model Training (Daily)**\n",
    "- **Automation**: Automated triggering, hyperparameter tuning, and metric calculation\n",
    "- **Human Oversight**: Review training results, approve hyperparameter changes, validate business logic\n",
    "- **Rationale**: While training can be automated, market regime changes require human expertise to interpret results and adjust strategies.\n",
    "\n",
    "**Model Validation (Daily)**\n",
    "- **Automation**: Statistical validation tests, performance metric calculation, A/B testing setup\n",
    "- **Human Oversight**: Business impact assessment, risk evaluation, deployment approval\n",
    "- **Rationale**: Model performance validation involves both statistical and business considerations that require human judgment.\n",
    "\n",
    "### **Manual with Automation Support**\n",
    "\n",
    "**Model Deployment (Weekly/On-Demand)**\n",
    "- **Automation**: Deployment infrastructure, health checks, rollback capabilities\n",
    "- **Manual Control**: Deployment timing, traffic routing decisions, risk assessment\n",
    "- **Rationale**: Model deployment to production has significant financial impact. Human approval ensures proper risk management and business alignment.\n",
    "\n",
    "**Incident Response & Model Rollbacks**\n",
    "- **Automation**: Incident detection, initial diagnostics, automated rollback triggers\n",
    "- **Manual Control**: Root cause analysis, fix implementation, go/no-go decisions\n",
    "- **Rationale**: Financial systems require human oversight for critical decisions, especially during incidents.\n",
    "\n",
    "### **Right-Sizing Justification**\n",
    "\n",
    "| **Automation Level** | **Tasks** | **Business Impact** | **Risk Level** | **Decision Driver** |\n",
    "|----------------------|-----------|-------------------|----------------|-------------------|\n",
    "| **Full Auto** | Data pipeline, serving | High availability | Low | Speed & consistency required |\n",
    "| **Semi-Auto** | Training, validation | Model quality | Medium | Expertise + efficiency balance |\n",
    "| **Manual** | Deployment, incidents | Financial safety | High | Risk management priority |\n",
    "\n",
    "### **Success Metrics**\n",
    "\n",
    "- **Data Pipeline**: 99.9% uptime, < 30s processing latency\n",
    "- **Model Training**: 95% automated success rate, human review within 2 hours\n",
    "- **Deployment**: Zero failed deployments, 100% human approval rate\n",
    "- **Incident Response**: < 5 minute detection, < 30 minute human response\n",
    "\n",
    "This strategy balances the need for high-frequency automation with appropriate human oversight for financial risk management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) (Stretch) Refactor One Task into a Function + CLI\n",
    "Use the templates below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Example: Feature Engineering Task\n",
      "==================================================\n",
      "2025-08-28 11:48:18,239 - INFO - [feature_engineering] Starting feature engineering task\n",
      "2025-08-28 11:48:18,256 - INFO - [feature_engineering] Loaded 100 records from data/sample_validated_data.parquet\n",
      "2025-08-28 11:48:18,259 - INFO - [feature_engineering] Created 10 features\n",
      "2025-08-28 11:48:18,259 - INFO - [feature_engineering] Saved features to data/features.parquet\n",
      "2025-08-28 11:48:18,259 - INFO - [feature_engineering] Saved metadata to data/features_metadata.json\n",
      "2025-08-28 11:48:18,259 - INFO - [feature_engineering] Completed in 0.02 seconds\n",
      "2025-08-28 11:48:18,260 - INFO - [main] Task feature_engineering completed successfully\n",
      "\n",
      "ğŸ¤– Example: Model Training Task\n",
      "==================================================\n",
      "2025-08-28 11:48:18,260 - INFO - [model_training] Starting regression model training\n",
      "2025-08-28 11:48:18,808 - INFO - [model_training] Loaded 100 records for training\n",
      "2025-08-28 11:48:18,810 - INFO - [model_training] Model performance: {'r2_score': 0.8992968676832939, 'rmse': 0.03879059837297919, 'model_type': 'regression'}\n",
      "2025-08-28 11:48:18,810 - INFO - [model_training] Saved model to models/trained_model.pkl\n",
      "2025-08-28 11:48:18,811 - INFO - [model_training] Saved report to models/trained_model_training_report.json\n",
      "2025-08-28 11:48:18,811 - INFO - [model_training] Completed in 0.55 seconds\n",
      "2025-08-28 11:48:18,811 - INFO - [main] Task model_training completed successfully\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# === FEATURE ENGINEERING TASK ===\n",
    "def feature_engineering_task(input_path: str, output_path: str, config_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Transform validated trading data into engineered features for model training.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to validated trading data (parquet)\n",
    "        output_path: Path to save engineered features (parquet)\n",
    "        config_path: Optional path to feature engineering configuration\n",
    "    \"\"\"\n",
    "    logging.info('[feature_engineering] Starting feature engineering task')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load validated trading data\n",
    "        if not Path(input_path).exists():\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "            \n",
    "        df = pd.read_parquet(input_path)\n",
    "        logging.info(f'[feature_engineering] Loaded {len(df)} records from {input_path}')\n",
    "        \n",
    "        # Load configuration if provided\n",
    "        config = {}\n",
    "        if config_path and Path(config_path).exists():\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "        \n",
    "        # Core feature engineering logic\n",
    "        features_df = df.copy()\n",
    "        \n",
    "        # 1. Calculate initiative buy/sell rate ratios\n",
    "        features_df['buy_sell_ratio'] = (\n",
    "            features_df['S_LI_INITIATIVEBUYRATE'] / \n",
    "            (features_df['S_LI_INITIATIVESELLRATE'] + 1e-8)\n",
    "        )\n",
    "        \n",
    "        # 2. Large order flow indicators\n",
    "        features_df['large_order_imbalance'] = (\n",
    "            features_df['S_LI_LARGEBUYRATE'] - features_df['S_LI_LARGESELLRATE']\n",
    "        )\n",
    "        \n",
    "        # 3. Rolling window features (if timestamp column exists)\n",
    "        if 'timestamp' in features_df.columns:\n",
    "            features_df = features_df.sort_values('timestamp')\n",
    "            # 5-minute rolling averages\n",
    "            for col in ['S_LI_INITIATIVEBUYRATE', 'S_LI_INITIATIVESELLRATE']:\n",
    "                features_df[f'{col}_5min_ma'] = features_df[col].rolling(window=5).mean()\n",
    "        \n",
    "        # 4. Market regime indicators\n",
    "        features_df['high_activity'] = (\n",
    "            (features_df['S_LI_INITIATIVEBUYRATE'] + features_df['S_LI_INITIATIVESELLRATE']) > 0.7\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Create output directory\n",
    "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save engineered features\n",
    "        features_df.to_parquet(output_path, index=False)\n",
    "        \n",
    "        # Generate metadata\n",
    "        metadata = {\n",
    "            'run_at': datetime.utcnow().isoformat(),\n",
    "            'input_path': input_path,\n",
    "            'output_path': output_path,\n",
    "            'input_records': len(df),\n",
    "            'output_records': len(features_df),\n",
    "            'features_created': list(set(features_df.columns) - set(df.columns)),\n",
    "            'processing_time_seconds': time.time() - start_time,\n",
    "            'config_used': config,\n",
    "            'data_quality': {\n",
    "                'null_rates': features_df.isnull().mean().to_dict(),\n",
    "                'feature_correlations': features_df.corr()['S_LI_INITIATIVESELLRATE'].to_dict()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = output_path.replace('.parquet', '_metadata.json')\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        logging.info(f'[feature_engineering] Created {len(features_df.columns)} features')\n",
    "        logging.info(f'[feature_engineering] Saved features to {output_path}')\n",
    "        logging.info(f'[feature_engineering] Saved metadata to {metadata_path}')\n",
    "        logging.info(f'[feature_engineering] Completed in {time.time() - start_time:.2f} seconds')\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'[feature_engineering] Error: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# === MODEL TRAINING TASK ===\n",
    "def model_training_task(input_path: str, output_path: str, model_type: str = 'regression') -> None:\n",
    "    \"\"\"\n",
    "    Train machine learning models on engineered features.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to engineered features (parquet)\n",
    "        output_path: Path to save trained model (pkl)\n",
    "        model_type: Type of model to train ('regression' or 'classification')\n",
    "    \"\"\"\n",
    "    logging.info(f'[model_training] Starting {model_type} model training')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, classification_report\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        import pickle\n",
    "        \n",
    "        # Load features\n",
    "        df = pd.read_parquet(input_path)\n",
    "        logging.info(f'[model_training] Loaded {len(df)} records for training')\n",
    "        \n",
    "        # Prepare features and target\n",
    "        feature_cols = [\n",
    "            'S_LI_INITIATIVEBUYRATE',\n",
    "            'S_LI_LARGEBUYRATE', \n",
    "            'S_LI_LARGESELLRATE',\n",
    "            'buy_sell_ratio',\n",
    "            'large_order_imbalance'\n",
    "        ]\n",
    "        \n",
    "        # Filter to available columns\n",
    "        available_features = [col for col in feature_cols if col in df.columns]\n",
    "        X = df[available_features].fillna(0)\n",
    "        y = df['S_LI_INITIATIVESELLRATE'].fillna(0)\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train model based on type\n",
    "        if model_type == 'regression':\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            metrics = {\n",
    "                'r2_score': float(r2_score(y_test, y_pred)),\n",
    "                'rmse': float(np.sqrt(mean_squared_error(y_test, y_pred))),\n",
    "                'model_type': 'regression'\n",
    "            }\n",
    "            \n",
    "        else:  # classification\n",
    "            # Convert to binary classification\n",
    "            y_binary = (y > y.median()).astype(int)\n",
    "            y_train_clf = y_binary[X_train.index]\n",
    "            y_test_clf = y_binary[X_test.index]\n",
    "            \n",
    "            model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', LogisticRegression(random_state=42))\n",
    "            ])\n",
    "            \n",
    "            model.fit(X_train, y_train_clf)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            metrics = {\n",
    "                'accuracy': float(accuracy_score(y_test_clf, y_pred)),\n",
    "                'classification_report': classification_report(y_test_clf, y_pred, output_dict=True),\n",
    "                'model_type': 'classification',\n",
    "                'threshold': float(y.median())\n",
    "            }\n",
    "        \n",
    "        # Create output directory\n",
    "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        model_data = {\n",
    "            'model': model,\n",
    "            'feature_names': available_features,\n",
    "            'metrics': metrics,\n",
    "            'training_metadata': {\n",
    "                'trained_at': datetime.utcnow().isoformat(),\n",
    "                'training_samples': len(X_train),\n",
    "                'test_samples': len(X_test),\n",
    "                'input_features': available_features\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        # Save training report\n",
    "        report_path = output_path.replace('.pkl', '_training_report.json')\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'metrics': metrics,\n",
    "                'training_metadata': model_data['training_metadata'],\n",
    "                'processing_time_seconds': time.time() - start_time\n",
    "            }, f, indent=2, default=str)\n",
    "        \n",
    "        logging.info(f'[model_training] Model performance: {metrics}')\n",
    "        logging.info(f'[model_training] Saved model to {output_path}')\n",
    "        logging.info(f'[model_training] Saved report to {report_path}')\n",
    "        logging.info(f'[model_training] Completed in {time.time() - start_time:.2f} seconds')\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'[model_training] Error: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# === CLI INTERFACE ===\n",
    "def main(argv=None):\n",
    "    parser = argparse.ArgumentParser(description='High-Frequency Trading Factor Pipeline Tasks')\n",
    "    parser.add_argument('task', choices=['feature_engineering', 'model_training'], \n",
    "                       help='Task to execute')\n",
    "    parser.add_argument('--input', required=True, help='Input file path')\n",
    "    parser.add_argument('--output', required=True, help='Output file path')\n",
    "    parser.add_argument('--config', help='Configuration file path')\n",
    "    parser.add_argument('--model-type', default='regression', \n",
    "                       choices=['regression', 'classification'],\n",
    "                       help='Model type for training task')\n",
    "    parser.add_argument('--log-level', default='INFO', \n",
    "                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],\n",
    "                       help='Logging level')\n",
    "    \n",
    "    args = parser.parse_args(argv)\n",
    "    \n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, args.log_level),\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "    \n",
    "    # Execute task\n",
    "    try:\n",
    "        if args.task == 'feature_engineering':\n",
    "            feature_engineering_task(args.input, args.output, args.config)\n",
    "        elif args.task == 'model_training':\n",
    "            model_training_task(args.input, args.output, args.model_type)\n",
    "            \n",
    "        logging.info(f'[main] Task {args.task} completed successfully')\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'[main] Task {args.task} failed: {str(e)}')\n",
    "        sys.exit(1)\n",
    "\n",
    "# === EXAMPLE USAGE ===\n",
    "if __name__ == '__main__':\n",
    "    # Simulate feature engineering task\n",
    "    print(\"ğŸ”§ Example: Feature Engineering Task\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create sample input data\n",
    "    sample_input = 'data/sample_validated_data.parquet'\n",
    "    Path(sample_input).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate sample trading data\n",
    "    np.random.seed(42)\n",
    "    sample_data = pd.DataFrame({\n",
    "        'S_LI_INITIATIVEBUYRATE': np.random.uniform(0.3, 0.7, 100),\n",
    "        'S_LI_INITIATIVESELLRATE': np.random.uniform(0.3, 0.7, 100),\n",
    "        'S_LI_LARGEBUYRATE': np.random.uniform(0.1, 0.3, 100),\n",
    "        'S_LI_LARGESELLRATE': np.random.uniform(0.1, 0.3, 100),\n",
    "        'timestamp': pd.date_range('2023-01-01', periods=100, freq='1min')\n",
    "    })\n",
    "    sample_data.to_parquet(sample_input, index=False)\n",
    "    \n",
    "    # Run feature engineering\n",
    "    main(['feature_engineering', '--input', sample_input, '--output', 'data/features.parquet'])\n",
    "    \n",
    "    print(\"\\nğŸ¤– Example: Model Training Task\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Run model training\n",
    "    main(['model_training', '--input', 'data/features.parquet', '--output', 'models/trained_model.pkl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Simple Retry Wrapper (fill in)\n",
    "Add a small retry with linear backoff to harden a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Testing Retry Mechanisms for Trading System\n",
      "============================================================\n",
      "2025-08-28 11:48:18,822 - INFO - [fetch_market_data] Fetching data from https://api.trading-data.com/live\n",
      "2025-08-28 11:48:18,823 - WARNING - [retry] fetch_market_data attempt 1/3 failed: Failed to connect to https://api.trading-data.com/live. Retrying in 0.44 seconds...\n",
      "2025-08-28 11:48:19,267 - INFO - [fetch_market_data] Fetching data from https://api.trading-data.com/live\n",
      "2025-08-28 11:48:19,268 - WARNING - [retry] fetch_market_data attempt 2/3 failed: Failed to connect to https://api.trading-data.com/live. Retrying in 0.95 seconds...\n",
      "2025-08-28 11:48:20,221 - INFO - [fetch_market_data] Fetching data from https://api.trading-data.com/live\n",
      "2025-08-28 11:48:20,221 - ERROR - [retry] fetch_market_data failed after 3 attempts: Failed to connect to https://api.trading-data.com/live\n",
      "âŒ Market data fetch failed: Failed to connect to https://api.trading-data.com/live\n",
      "2025-08-28 11:48:20,222 - INFO - [process_trading_signals] Processing trading signals\n",
      "âœ… Signal processing successful: 3 signals generated\n",
      "\n",
      " Circuit Breaker Status:\n",
      "State: CLOSED\n",
      "Failure Count: 1\n",
      "Last Failure: 1756396100.2222478\n",
      "\n",
      " Production Considerations:\n",
      "â€¢ Retry mechanisms prevent transient failures from disrupting trading\n",
      "â€¢ Circuit breakers protect against cascading failures during market stress\n",
      "â€¢ Exponential backoff with jitter prevents thundering herd effects\n",
      "â€¢ Structured logging enables rapid incident response\n",
      "â€¢ Configuration allows tuning for different criticality levels\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import logging\n",
    "from functools import wraps\n",
    "from typing import Callable, Any, Type, Tuple\n",
    "\n",
    "def retry(n_tries: int = 3, \n",
    "          delay: float = 0.2, \n",
    "          backoff: float = 2.0, \n",
    "          jitter: bool = True,\n",
    "          exceptions: Tuple[Type[Exception], ...] = (Exception,)):\n",
    "    \"\"\"\n",
    "    Advanced retry decorator with exponential backoff and jitter for production trading systems.\n",
    "    \n",
    "    Args:\n",
    "        n_tries: Number of retry attempts\n",
    "        delay: Initial delay between retries (seconds)\n",
    "        backoff: Exponential backoff multiplier\n",
    "        jitter: Add random jitter to prevent thundering herd\n",
    "        exceptions: Tuple of exceptions to catch and retry\n",
    "    \"\"\"\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs) -> Any:\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(n_tries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                    \n",
    "                except exceptions as e:\n",
    "                    last_exception = e\n",
    "                    \n",
    "                    if attempt == n_tries - 1:  # Last attempt\n",
    "                        logging.error(f'[retry] {func.__name__} failed after {n_tries} attempts: {str(e)}')\n",
    "                        raise e\n",
    "                    \n",
    "                    # Calculate delay with exponential backoff\n",
    "                    current_delay = delay * (backoff ** attempt)\n",
    "                    \n",
    "                    # Add jitter to prevent thundering herd\n",
    "                    if jitter:\n",
    "                        current_delay *= (0.5 + random.random())\n",
    "                    \n",
    "                    logging.warning(f'[retry] {func.__name__} attempt {attempt + 1}/{n_tries} failed: {str(e)}. '\n",
    "                                  f'Retrying in {current_delay:.2f} seconds...')\n",
    "                    \n",
    "                    time.sleep(current_delay)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Non-retryable exception\n",
    "                    logging.error(f'[retry] {func.__name__} failed with non-retryable exception: {str(e)}')\n",
    "                    raise e\n",
    "            \n",
    "            # Should never reach here, but just in case\n",
    "            raise last_exception\n",
    "            \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# === CIRCUIT BREAKER PATTERN ===\n",
    "class CircuitBreaker:\n",
    "    \"\"\"\n",
    "    Circuit breaker pattern for high-frequency trading system resilience.\n",
    "    Prevents cascading failures by failing fast when error rates are high.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 failure_threshold: int = 5,\n",
    "                 recovery_timeout: float = 60.0,\n",
    "                 expected_exception: Type[Exception] = Exception):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.recovery_timeout = recovery_timeout\n",
    "        self.expected_exception = expected_exception\n",
    "        \n",
    "        # State tracking\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN\n",
    "        \n",
    "    def call(self, func: Callable, *args, **kwargs) -> Any:\n",
    "        \"\"\"Execute function with circuit breaker protection\"\"\"\n",
    "        \n",
    "        if self.state == 'OPEN':\n",
    "            if self._should_attempt_reset():\n",
    "                self.state = 'HALF_OPEN'\n",
    "                logging.info(f'[circuit_breaker] Attempting to reset circuit for {func.__name__}')\n",
    "            else:\n",
    "                raise Exception(f'Circuit breaker OPEN for {func.__name__}. Last failure: {self.last_failure_time}')\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            self._on_success()\n",
    "            return result\n",
    "            \n",
    "        except self.expected_exception as e:\n",
    "            self._on_failure()\n",
    "            raise e\n",
    "    \n",
    "    def _should_attempt_reset(self) -> bool:\n",
    "        \"\"\"Check if enough time has passed to attempt reset\"\"\"\n",
    "        return (time.time() - self.last_failure_time) >= self.recovery_timeout\n",
    "    \n",
    "    def _on_success(self):\n",
    "        \"\"\"Handle successful execution\"\"\"\n",
    "        self.failure_count = 0\n",
    "        self.state = 'CLOSED'\n",
    "        \n",
    "    def _on_failure(self):\n",
    "        \"\"\"Handle failed execution\"\"\"\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        \n",
    "        if self.failure_count >= self.failure_threshold:\n",
    "            self.state = 'OPEN'\n",
    "            logging.warning(f'[circuit_breaker] Circuit opened after {self.failure_count} failures')\n",
    "\n",
    "# === EXAMPLE USAGE WITH TRADING TASKS ===\n",
    "\n",
    "# Create circuit breaker for external API calls\n",
    "api_circuit_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30.0)\n",
    "\n",
    "@retry(n_tries=3, delay=0.5, exceptions=(ConnectionError, TimeoutError))\n",
    "def fetch_market_data(source_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Simulate fetching market data with retry and circuit breaker protection.\n",
    "    \"\"\"\n",
    "    logging.info(f'[fetch_market_data] Fetching data from {source_url}')\n",
    "    \n",
    "    # Simulate network issues\n",
    "    if random.random() < 0.3:  # 30% chance of failure\n",
    "        raise ConnectionError(f\"Failed to connect to {source_url}\")\n",
    "    \n",
    "    # Simulate successful data fetch\n",
    "    return {\n",
    "        'timestamp': time.time(),\n",
    "        'data': f'market_data_from_{source_url}',\n",
    "        'status': 'success'\n",
    "    }\n",
    "\n",
    "def safe_fetch_market_data(source_url: str) -> dict:\n",
    "    \"\"\"Market data fetch with circuit breaker protection\"\"\"\n",
    "    return api_circuit_breaker.call(fetch_market_data, source_url)\n",
    "\n",
    "@retry(n_tries=2, delay=1.0, exceptions=(ValueError, RuntimeError))\n",
    "def process_trading_signals(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Process trading signals with retry for transient failures.\n",
    "    \"\"\"\n",
    "    logging.info('[process_trading_signals] Processing trading signals')\n",
    "    \n",
    "    if not data or 'data' not in data:\n",
    "        raise ValueError(\"Invalid data format\")\n",
    "    \n",
    "    # Simulate processing\n",
    "    if random.random() < 0.2:  # 20% chance of processing error\n",
    "        raise RuntimeError(\"Signal processing error\")\n",
    "    \n",
    "    return {\n",
    "        'signals': ['BUY', 'HOLD', 'SELL'],\n",
    "        'confidence': random.uniform(0.7, 0.95),\n",
    "        'processed_at': time.time()\n",
    "    }\n",
    "\n",
    "# === DEMONSTRATION ===\n",
    "print(\"ğŸ”„ Testing Retry Mechanisms for Trading System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test retry with success after failures\n",
    "try:\n",
    "    result = safe_fetch_market_data(\"https://api.trading-data.com/live\")\n",
    "    print(f\"âœ… Market data fetch successful: {result['status']}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Market data fetch failed: {str(e)}\")\n",
    "\n",
    "# Test processing with retry\n",
    "try:\n",
    "    sample_data = {'data': 'trading_signals', 'timestamp': time.time()}\n",
    "    signals = process_trading_signals(sample_data)\n",
    "    print(f\"âœ… Signal processing successful: {len(signals['signals'])} signals generated\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Signal processing failed: {str(e)}\")\n",
    "\n",
    "print(\"\\n Circuit Breaker Status:\")\n",
    "print(f\"State: {api_circuit_breaker.state}\")\n",
    "print(f\"Failure Count: {api_circuit_breaker.failure_count}\")\n",
    "print(f\"Last Failure: {api_circuit_breaker.last_failure_time}\")\n",
    "\n",
    "print(\"\\n Production Considerations:\")\n",
    "print(\"â€¢ Retry mechanisms prevent transient failures from disrupting trading\")\n",
    "print(\"â€¢ Circuit breakers protect against cascading failures during market stress\")\n",
    "print(\"â€¢ Exponential backoff with jitter prevents thundering herd effects\")\n",
    "print(\"â€¢ Structured logging enables rapid incident response\")\n",
    "print(\"â€¢ Configuration allows tuning for different criticality levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631543d9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
